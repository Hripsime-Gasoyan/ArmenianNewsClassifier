{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "337e6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import hstack\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d75b95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned=pd.read_csv(r'data_cleaned.csv') # Importing data where the texts are translated and cleaned from stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ea2436a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_title(title):\n",
    "#     \"\"\"\n",
    "#     The function is tokenizing text into separate tokens, in this case words\n",
    "#     \"\"\"\n",
    "#     tokenized_title = tokenizer.tokenize(title)\n",
    "#     return tokenized_title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17625749",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') #Pretrained sentence transformer to get embeddings for text columns\n",
    "\n",
    "data_cleaned['Text_vector']=data_cleaned['Text_first200_translated'].apply(lambda x :model.encode(x))\n",
    "data_cleaned['Title_vector']=data_cleaned['Title_Translated'].apply(lambda x :model.encode(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23f63852",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an instance of LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit LabelEncoder on the \"Category\" column\n",
    "label_encoder.fit(data_cleaned['Category'])\n",
    "\n",
    "# Transform the \"Category\" column to numerical classes\n",
    "data_cleaned['Category_2'] = label_encoder.transform(data_cleaned['Category'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f2a4384c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Մշակույթ            739\n",
       "Տնտեսություն        719\n",
       "Կորոնավիրուս        645\n",
       "Սպորտ               575\n",
       "Քաղաքականություն    515\n",
       "ՏՏ ոլորտ            187\n",
       "Name: Category, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c56642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    739\n",
       "4    719\n",
       "0    645\n",
       "2    575\n",
       "5    515\n",
       "3    187\n",
       "Name: Category_2, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned['Category_2'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2498e062",
   "metadata": {},
   "source": [
    "### Model using first 200 words of text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6a8ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(data_cleaned, random_state=42, test_size=0.3)\n",
    "\n",
    "# Extract the feature columns and target column for the training set\n",
    "X_train = train.Text_first200_translated_cleaned\n",
    "y_train = train.Category_2\n",
    "\n",
    "# Extract the feature columns and target column for the testing set\n",
    "X_test = test.Text_first200_translated_cleaned\n",
    "y_test = test.Category_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1a177e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=1, stop_words='english')\n",
    "\n",
    "# Convert the training set text data to TF-IDF vectors\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Convert the testing set text data to TF-IDF vectors\n",
    "test_vectors = vectorizer.transform(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6985c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp.fit(train_vectors, y_train)\n",
    "\n",
    "# Predict the target labels for the testing data\n",
    "mlp_prediction = mlp.predict(test_vectors)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ec1bb8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398312998149743"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the F1 score using weighted averaging\n",
    "f1 = f1_score(y_test, mlp_prediction, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c32dfe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9398422090729783"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, mlp_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f885a64e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3273    AmericanArmenian TV star Kim Kardashian visit ...\n",
       "900     Every year various cultural centers museums li...\n",
       "422     epidemic situation also tense Artsakh number m...\n",
       "887     Film production complicated process especially...\n",
       "1825    morning capital Ukraine targeted drones Presid...\n",
       "                              ...                        \n",
       "1095    future museums restore rethink title Internati...\n",
       "1130    Yesterday evening biggest theater event year t...\n",
       "1294    known recent years publishing processes number...\n",
       "860     Armenia Art Fair2022 art fair opens today Yere...\n",
       "3174    Minutes ago Lille Slovakian football team beat...\n",
       "Name: Text_first200_translated_cleaned, Length: 2366, dtype: object"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d5a62ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Title</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_first200</th>\n",
       "      <th>Text_first200_translated</th>\n",
       "      <th>Title_Translated</th>\n",
       "      <th>Text_first200_translated_cleaned</th>\n",
       "      <th>Title_Translated_cleaned</th>\n",
       "      <th>Text_vector</th>\n",
       "      <th>Title_vector</th>\n",
       "      <th>Category_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Կորոնավիրուս</td>\n",
       "      <td>ԱՀԿ-ը Չինաստանին խնդրում է համագործակցել COVID...</td>\n",
       "      <td>Առողջապահության համաշխարհային կազմակերպությու...</td>\n",
       "      <td>Առողջապահության համաշխարհային կազմակերպություն...</td>\n",
       "      <td>The World Health Organization (WHO) is confide...</td>\n",
       "      <td>WHO asks China to cooperate in identifying the...</td>\n",
       "      <td>World Health Organization confident China sign...</td>\n",
       "      <td>asks China cooperate identifying origin COVID19</td>\n",
       "      <td>[-0.081531666, 0.049715642, -0.03568201, 0.048...</td>\n",
       "      <td>[-0.08257137, 0.05076738, -0.015521194, -0.012...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Կորոնավիրուս</td>\n",
       "      <td>ԱՄՆ նախագահը ստորագրեց COVID-19-ի արտակարգ դրո...</td>\n",
       "      <td>Միացյալ Նահանգների նախագահ Ջո Բայդենը հավանու...</td>\n",
       "      <td>Միացյալ Նահանգների նախագահ Ջո Բայդենը հավանութ...</td>\n",
       "      <td>The President of the United States, Joe Biden,...</td>\n",
       "      <td>The President of the United States signed the ...</td>\n",
       "      <td>President United States Joe Biden approved lif...</td>\n",
       "      <td>President United States signed law end state e...</td>\n",
       "      <td>[-0.02142138, 0.058397654, 0.09130869, 0.01095...</td>\n",
       "      <td>[-0.03281866, 0.041443933, 0.024626661, -0.010...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Կորոնավիրուս</td>\n",
       "      <td>ԱՄՆ հետախուզությանը հանձնարարվել է կորոնավիրու...</td>\n",
       "      <td>Միացյալ Նահանգների նախագահ Ջո Բայդենը երեկ օր...</td>\n",
       "      <td>Միացյալ Նահանգների նախագահ Ջո Բայդենը երեկ օրե...</td>\n",
       "      <td>United States President Joe Biden signed a law...</td>\n",
       "      <td>US intelligence has been instructed to declass...</td>\n",
       "      <td>United States President Joe Biden signed law y...</td>\n",
       "      <td>US intelligence instructed declassify informat...</td>\n",
       "      <td>[-0.0761797, 0.08938678, 0.027588481, -0.04014...</td>\n",
       "      <td>[-0.04919257, 0.05779532, -0.03231708, 0.00079...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Կորոնավիրուս</td>\n",
       "      <td>ՀԴԲ տնօրեն. Covid-19-ը ենթադրաբար ծագել է չինա...</td>\n",
       "      <td>Միացյալ Նահանգների Հետաքննությունների դաշնայի...</td>\n",
       "      <td>Միացյալ Նահանգների Հետաքննությունների դաշնային...</td>\n",
       "      <td>The director of the Federal Bureau of Investig...</td>\n",
       "      <td>Director of the FBI. Covid-19 is believed to h...</td>\n",
       "      <td>director Federal Bureau Investigation FBI Unit...</td>\n",
       "      <td>Director FBI Covid19 believed originated labor...</td>\n",
       "      <td>[-0.09603007, 0.060622435, -0.030653626, 0.054...</td>\n",
       "      <td>[-0.08993958, -0.0047365096, -0.08058686, 0.00...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Կորոնավիրուս</td>\n",
       "      <td>ԱՄՆ-ում նորից խոսում են կորոնավիրուսի «չինական...</td>\n",
       "      <td>Միացյալ Նահանգներում կենսաբանական լաբորատորիա...</td>\n",
       "      <td>Միացյալ Նահանգներում կենսաբանական լաբորատորիան...</td>\n",
       "      <td>The Department of Energy, which oversees the a...</td>\n",
       "      <td>In the USA, they are again talking about the \"...</td>\n",
       "      <td>Department Energy oversees activities biologic...</td>\n",
       "      <td>USA talking Chinese trace coronavirus</td>\n",
       "      <td>[-0.047141563, 0.0538676, 0.012197902, 0.07695...</td>\n",
       "      <td>[-0.014239151, 0.028358832, 0.0043151435, 0.05...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3375</th>\n",
       "      <td>3375</td>\n",
       "      <td>ՏՏ ոլորտ</td>\n",
       "      <td>«Համադասարանցիներ» սոցիալական ցանցը ուղիղ վիդե...</td>\n",
       "      <td>Toyota-ն և Uber-ը ռազմավարական գործընկերությո...</td>\n",
       "      <td>Toyota-ն և Uber-ը ռազմավարական գործընկերությու...</td>\n",
       "      <td>Toyota and Uber have formed a strategic partne...</td>\n",
       "      <td>The \"Classmates\" social network is launching a...</td>\n",
       "      <td>Toyota Uber formed strategic partnership Japan...</td>\n",
       "      <td>Classmates social network launching live video...</td>\n",
       "      <td>[-0.0033210143, -0.032762624, 0.014608071, -0....</td>\n",
       "      <td>[-0.0063318717, -0.079445325, -0.044393063, -0...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3376</th>\n",
       "      <td>3376</td>\n",
       "      <td>ՏՏ ոլորտ</td>\n",
       "      <td>Այգեկ գյուղի ինժեներական լաբորատորիայում երեխա...</td>\n",
       "      <td>Միացյալ Նահանգների Հետաքննությունների դաշնայի...</td>\n",
       "      <td>Միացյալ Նահանգների Հետաքննությունների դաշնային...</td>\n",
       "      <td>The United States Federal Bureau of Investigat...</td>\n",
       "      <td>In the engineering laboratory of Aygek village...</td>\n",
       "      <td>United States Federal Bureau Investigation dro...</td>\n",
       "      <td>engineering laboratory Aygek village children ...</td>\n",
       "      <td>[-0.08302667, 0.053029276, -0.008729843, -0.03...</td>\n",
       "      <td>[-0.08819589, 0.03656708, -0.08492272, 0.04892...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3377</th>\n",
       "      <td>3377</td>\n",
       "      <td>ՏՏ ոլորտ</td>\n",
       "      <td>Twitter-ը հայտարարում է երկար սպասված փոփոխութ...</td>\n",
       "      <td>Դատարանում Միացյալ Նահանգների արդարադատության...</td>\n",
       "      <td>Դատարանում Միացյալ Նահանգների արդարադատության ...</td>\n",
       "      <td>After yesterday's defeat by the United States ...</td>\n",
       "      <td>Twitter is announcing some long-awaited changes</td>\n",
       "      <td>yesterdays defeat United States Department Jus...</td>\n",
       "      <td>Twitter announcing longawaited changes</td>\n",
       "      <td>[-0.07224804, 0.035403356, 0.0068919775, -0.06...</td>\n",
       "      <td>[-0.04011054, 0.012772688, 0.05832622, -0.0057...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3378</th>\n",
       "      <td>3378</td>\n",
       "      <td>ՏՏ ոլորտ</td>\n",
       "      <td>Toyota-ն և Uber-ը ռազմավարական գործընկերությու...</td>\n",
       "      <td>1989 թվականին թողարկված «Վերադարձ դեպի ապագա ...</td>\n",
       "      <td>1989 թվականին թողարկված «Վերադարձ դեպի ապագա 2...</td>\n",
       "      <td>The hero of the 1989 science fiction film \"Bac...</td>\n",
       "      <td>Toyota and Uber have formed a strategic partne...</td>\n",
       "      <td>hero 1989 science fiction film Back Future 2 a...</td>\n",
       "      <td>Toyota Uber formed strategic partnership</td>\n",
       "      <td>[-0.112825565, 0.006945745, 0.01304844, -0.030...</td>\n",
       "      <td>[0.047724172, 0.010241674, -0.03746954, -0.067...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3379</th>\n",
       "      <td>3379</td>\n",
       "      <td>ՏՏ ոլորտ</td>\n",
       "      <td>Ամերիկացի իրավապահները Սան Բերնարդինոյի ահաբեկ...</td>\n",
       "      <td>Ամերիկացի դեռահասների շրջանում Piper Jaffray ...</td>\n",
       "      <td>Ամերիկացի դեռահասների շրջանում Piper Jaffray ն...</td>\n",
       "      <td>According to a survey of American teenagers by...</td>\n",
       "      <td>American law enforcement officers hacked the S...</td>\n",
       "      <td>According survey American teenagers investment...</td>\n",
       "      <td>American law enforcement officers hacked San B...</td>\n",
       "      <td>[0.015026469, -0.01635725, -0.015510105, -0.03...</td>\n",
       "      <td>[-0.034380596, 0.039474104, -0.01707229, -0.03...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3380 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      Category  \\\n",
       "0              0  Կորոնավիրուս   \n",
       "1              1  Կորոնավիրուս   \n",
       "2              2  Կորոնավիրուս   \n",
       "3              3  Կորոնավիրուս   \n",
       "4              4  Կորոնավիրուս   \n",
       "...          ...           ...   \n",
       "3375        3375      ՏՏ ոլորտ   \n",
       "3376        3376      ՏՏ ոլորտ   \n",
       "3377        3377      ՏՏ ոլորտ   \n",
       "3378        3378      ՏՏ ոլորտ   \n",
       "3379        3379      ՏՏ ոլորտ   \n",
       "\n",
       "                                                  Title  \\\n",
       "0     ԱՀԿ-ը Չինաստանին խնդրում է համագործակցել COVID...   \n",
       "1     ԱՄՆ նախագահը ստորագրեց COVID-19-ի արտակարգ դրո...   \n",
       "2     ԱՄՆ հետախուզությանը հանձնարարվել է կորոնավիրու...   \n",
       "3     ՀԴԲ տնօրեն. Covid-19-ը ենթադրաբար ծագել է չինա...   \n",
       "4     ԱՄՆ-ում նորից խոսում են կորոնավիրուսի «չինական...   \n",
       "...                                                 ...   \n",
       "3375  «Համադասարանցիներ» սոցիալական ցանցը ուղիղ վիդե...   \n",
       "3376  Այգեկ գյուղի ինժեներական լաբորատորիայում երեխա...   \n",
       "3377  Twitter-ը հայտարարում է երկար սպասված փոփոխութ...   \n",
       "3378  Toyota-ն և Uber-ը ռազմավարական գործընկերությու...   \n",
       "3379  Ամերիկացի իրավապահները Սան Բերնարդինոյի ահաբեկ...   \n",
       "\n",
       "                                                   Text  \\\n",
       "0      Առողջապահության համաշխարհային կազմակերպությու...   \n",
       "1      Միացյալ Նահանգների նախագահ Ջո Բայդենը հավանու...   \n",
       "2      Միացյալ Նահանգների նախագահ Ջո Բայդենը երեկ օր...   \n",
       "3      Միացյալ Նահանգների Հետաքննությունների դաշնայի...   \n",
       "4      Միացյալ Նահանգներում կենսաբանական լաբորատորիա...   \n",
       "...                                                 ...   \n",
       "3375   Toyota-ն և Uber-ը ռազմավարական գործընկերությո...   \n",
       "3376   Միացյալ Նահանգների Հետաքննությունների դաշնայի...   \n",
       "3377   Դատարանում Միացյալ Նահանգների արդարադատության...   \n",
       "3378   1989 թվականին թողարկված «Վերադարձ դեպի ապագա ...   \n",
       "3379   Ամերիկացի դեռահասների շրջանում Piper Jaffray ...   \n",
       "\n",
       "                                          Text_first200  \\\n",
       "0     Առողջապահության համաշխարհային կազմակերպություն...   \n",
       "1     Միացյալ Նահանգների նախագահ Ջո Բայդենը հավանութ...   \n",
       "2     Միացյալ Նահանգների նախագահ Ջո Բայդենը երեկ օրե...   \n",
       "3     Միացյալ Նահանգների Հետաքննությունների դաշնային...   \n",
       "4     Միացյալ Նահանգներում կենսաբանական լաբորատորիան...   \n",
       "...                                                 ...   \n",
       "3375  Toyota-ն և Uber-ը ռազմավարական գործընկերությու...   \n",
       "3376  Միացյալ Նահանգների Հետաքննությունների դաշնային...   \n",
       "3377  Դատարանում Միացյալ Նահանգների արդարադատության ...   \n",
       "3378  1989 թվականին թողարկված «Վերադարձ դեպի ապագա 2...   \n",
       "3379  Ամերիկացի դեռահասների շրջանում Piper Jaffray ն...   \n",
       "\n",
       "                               Text_first200_translated  \\\n",
       "0     The World Health Organization (WHO) is confide...   \n",
       "1     The President of the United States, Joe Biden,...   \n",
       "2     United States President Joe Biden signed a law...   \n",
       "3     The director of the Federal Bureau of Investig...   \n",
       "4     The Department of Energy, which oversees the a...   \n",
       "...                                                 ...   \n",
       "3375  Toyota and Uber have formed a strategic partne...   \n",
       "3376  The United States Federal Bureau of Investigat...   \n",
       "3377  After yesterday's defeat by the United States ...   \n",
       "3378  The hero of the 1989 science fiction film \"Bac...   \n",
       "3379  According to a survey of American teenagers by...   \n",
       "\n",
       "                                       Title_Translated  \\\n",
       "0     WHO asks China to cooperate in identifying the...   \n",
       "1     The President of the United States signed the ...   \n",
       "2     US intelligence has been instructed to declass...   \n",
       "3     Director of the FBI. Covid-19 is believed to h...   \n",
       "4     In the USA, they are again talking about the \"...   \n",
       "...                                                 ...   \n",
       "3375  The \"Classmates\" social network is launching a...   \n",
       "3376  In the engineering laboratory of Aygek village...   \n",
       "3377    Twitter is announcing some long-awaited changes   \n",
       "3378  Toyota and Uber have formed a strategic partne...   \n",
       "3379  American law enforcement officers hacked the S...   \n",
       "\n",
       "                       Text_first200_translated_cleaned  \\\n",
       "0     World Health Organization confident China sign...   \n",
       "1     President United States Joe Biden approved lif...   \n",
       "2     United States President Joe Biden signed law y...   \n",
       "3     director Federal Bureau Investigation FBI Unit...   \n",
       "4     Department Energy oversees activities biologic...   \n",
       "...                                                 ...   \n",
       "3375  Toyota Uber formed strategic partnership Japan...   \n",
       "3376  United States Federal Bureau Investigation dro...   \n",
       "3377  yesterdays defeat United States Department Jus...   \n",
       "3378  hero 1989 science fiction film Back Future 2 a...   \n",
       "3379  According survey American teenagers investment...   \n",
       "\n",
       "                               Title_Translated_cleaned  \\\n",
       "0       asks China cooperate identifying origin COVID19   \n",
       "1     President United States signed law end state e...   \n",
       "2     US intelligence instructed declassify informat...   \n",
       "3     Director FBI Covid19 believed originated labor...   \n",
       "4                 USA talking Chinese trace coronavirus   \n",
       "...                                                 ...   \n",
       "3375  Classmates social network launching live video...   \n",
       "3376  engineering laboratory Aygek village children ...   \n",
       "3377             Twitter announcing longawaited changes   \n",
       "3378           Toyota Uber formed strategic partnership   \n",
       "3379  American law enforcement officers hacked San B...   \n",
       "\n",
       "                                            Text_vector  \\\n",
       "0     [-0.081531666, 0.049715642, -0.03568201, 0.048...   \n",
       "1     [-0.02142138, 0.058397654, 0.09130869, 0.01095...   \n",
       "2     [-0.0761797, 0.08938678, 0.027588481, -0.04014...   \n",
       "3     [-0.09603007, 0.060622435, -0.030653626, 0.054...   \n",
       "4     [-0.047141563, 0.0538676, 0.012197902, 0.07695...   \n",
       "...                                                 ...   \n",
       "3375  [-0.0033210143, -0.032762624, 0.014608071, -0....   \n",
       "3376  [-0.08302667, 0.053029276, -0.008729843, -0.03...   \n",
       "3377  [-0.07224804, 0.035403356, 0.0068919775, -0.06...   \n",
       "3378  [-0.112825565, 0.006945745, 0.01304844, -0.030...   \n",
       "3379  [0.015026469, -0.01635725, -0.015510105, -0.03...   \n",
       "\n",
       "                                           Title_vector  Category_2  \n",
       "0     [-0.08257137, 0.05076738, -0.015521194, -0.012...           0  \n",
       "1     [-0.03281866, 0.041443933, 0.024626661, -0.010...           0  \n",
       "2     [-0.04919257, 0.05779532, -0.03231708, 0.00079...           0  \n",
       "3     [-0.08993958, -0.0047365096, -0.08058686, 0.00...           0  \n",
       "4     [-0.014239151, 0.028358832, 0.0043151435, 0.05...           0  \n",
       "...                                                 ...         ...  \n",
       "3375  [-0.0063318717, -0.079445325, -0.044393063, -0...           3  \n",
       "3376  [-0.08819589, 0.03656708, -0.08492272, 0.04892...           3  \n",
       "3377  [-0.04011054, 0.012772688, 0.05832622, -0.0057...           3  \n",
       "3378  [0.047724172, 0.010241674, -0.03746954, -0.067...           3  \n",
       "3379  [-0.034380596, 0.039474104, -0.01707229, -0.03...           3  \n",
       "\n",
       "[3380 rows x 12 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d471f",
   "metadata": {},
   "source": [
    "### Model Using title and first 200 words of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fb613249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(data_cleaned, random_state=42, test_size=0.3)\n",
    "\n",
    "# Concatenate the two text columns for the training set\n",
    "X_train = train['Text_first200_translated_cleaned'] + ' ' + train['Title_Translated_cleaned']\n",
    "\n",
    "# Concatenate the two text columns for the testing set\n",
    "X_test = test['Text_first200_translated_cleaned'] + ' ' + test['Title_Translated_cleaned']\n",
    "\n",
    "# Extract the target column for the training and testing sets\n",
    "y_train = train['Category_2']\n",
    "y_test = test['Category_2']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64f12a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=0.9, min_df=1, stop_words='english')\n",
    "\n",
    "# Convert the training set text data to TF-IDF vectors\n",
    "train_vectors = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Convert the testing set text data to TF-IDF vectors\n",
    "test_vectors = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0421a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp.fit(train_vectors, y_train)\n",
    "\n",
    "# Predict the target labels for the testing data\n",
    "mlp_prediction = mlp.predict(test_vectors)\n",
    "\n",
    "# Calculate the F1 score using weighted averaging\n",
    "f1 = f1_score(y_test, mlp_prediction, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9fbda710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9505752142533993"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "169441e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9506903353057199"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9ec884",
   "metadata": {},
   "source": [
    "### Model Using title and first 200 words of text separately\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6dfcdbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(data_cleaned, random_state=42, test_size=0.3)\n",
    "\n",
    "X_train = train[['Text_first200_translated_cleaned','Title_Translated_cleaned']]\n",
    "\n",
    "X_test = test[['Text_first200_translated_cleaned','Title_Translated_cleaned']]\n",
    "\n",
    "y_train = train['Category_2']\n",
    "y_test = test['Category_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "11134601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text_first200_translated_cleaned</th>\n",
       "      <th>Title_Translated_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>AmericanArmenian TV star Kim Kardashian visit ...</td>\n",
       "      <td>DataArt company also operate Yerevan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>Every year various cultural centers museums li...</td>\n",
       "      <td>World Poetry Day celebrated Armenia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>epidemic situation also tense Artsakh number m...</td>\n",
       "      <td>Lets infect others others infect us Vaccinatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>Film production complicated process especially...</td>\n",
       "      <td>Jivan Avetisyans film Heavens Gate shown Ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>morning capital Ukraine targeted drones Presid...</td>\n",
       "      <td>corruption scandal European Parliament abating</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>future museums restore rethink title Internati...</td>\n",
       "      <td>113 museums Armenia Artsakh involved events In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>Yesterday evening biggest theater event year t...</td>\n",
       "      <td>best known Artavazd 20th award ceremony held</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>known recent years publishing processes number...</td>\n",
       "      <td>Dont Kill Tomorrows Book initiative seeing suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>Armenia Art Fair2022 art fair opens today Yere...</td>\n",
       "      <td>Armenia Art Fair2022 modern art fair starting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>Minutes ago Lille Slovakian football team beat...</td>\n",
       "      <td>Lavrov Provocative actions fans countries ignored</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2366 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Text_first200_translated_cleaned  \\\n",
       "3273  AmericanArmenian TV star Kim Kardashian visit ...   \n",
       "900   Every year various cultural centers museums li...   \n",
       "422   epidemic situation also tense Artsakh number m...   \n",
       "887   Film production complicated process especially...   \n",
       "1825  morning capital Ukraine targeted drones Presid...   \n",
       "...                                                 ...   \n",
       "1095  future museums restore rethink title Internati...   \n",
       "1130  Yesterday evening biggest theater event year t...   \n",
       "1294  known recent years publishing processes number...   \n",
       "860   Armenia Art Fair2022 art fair opens today Yere...   \n",
       "3174  Minutes ago Lille Slovakian football team beat...   \n",
       "\n",
       "                               Title_Translated_cleaned  \n",
       "3273               DataArt company also operate Yerevan  \n",
       "900                 World Poetry Day celebrated Armenia  \n",
       "422   Lets infect others others infect us Vaccinatio...  \n",
       "887   Jivan Avetisyans film Heavens Gate shown Ameri...  \n",
       "1825     corruption scandal European Parliament abating  \n",
       "...                                                 ...  \n",
       "1095  113 museums Armenia Artsakh involved events In...  \n",
       "1130       best known Artavazd 20th award ceremony held  \n",
       "1294  Dont Kill Tomorrows Book initiative seeing suc...  \n",
       "860       Armenia Art Fair2022 modern art fair starting  \n",
       "3174  Lavrov Provocative actions fans countries ignored  \n",
       "\n",
       "[2366 rows x 2 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0fa1cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TF-IDF vectorizer\n",
    "vectorizer1 = TfidfVectorizer(max_df=0.9, min_df=1, stop_words='english')\n",
    "\n",
    "# Convert the training set text data to TF-IDF vectors\n",
    "train_vectors1 = vectorizer1.fit_transform(X_train['Text_first200_translated_cleaned'])\n",
    "\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(max_df=0.9, min_df=1, stop_words='english')\n",
    "\n",
    "# Convert the training set text data to TF-IDF vectors\n",
    "train_vectors2 = vectorizer2.fit_transform(X_train['Title_Translated_cleaned'])\n",
    "# train_vectors2 = vectorizer.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Convert the testing set text data to TF-IDF vectors\n",
    "test_vectors1 = vectorizer1.transform(X_test['Text_first200_translated_cleaned'])\n",
    "test_vectors2 = vectorizer2.transform(X_test['Title_Translated_cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "29506c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_vectors_combined = hstack((train_vectors1, train_vectors2))\n",
    "test_vectors_combined = hstack((test_vectors1, test_vectors2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25239af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp.fit(train_vectors_combined, y_train_reshaped)\n",
    "\n",
    "# Predict the target labels for the testing data\n",
    "mlp_prediction = mlp.predict(test_vectors_combined)\n",
    "\n",
    "# Calculate the F1 score using weighted averaging\n",
    "f1 = f1_score(y_test, mlp_prediction, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "24c970e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497501888891813"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "481b351e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9497041420118343"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, mlp_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8e10b4",
   "metadata": {},
   "source": [
    "### Bert Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f24a9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(data_cleaned, random_state=42, test_size=0.3)\n",
    "train_features=train['Text_vector'].apply(pd.Series)\n",
    "test_features=test['Text_vector'].apply(pd.Series)\n",
    "\n",
    "# Extract the feature columns and target column for the training set\n",
    "X_train = train_features\n",
    "y_train = train.Category_2\n",
    "\n",
    "# Extract the feature columns and target column for the testing set\n",
    "X_test = test_features\n",
    "y_test = test.Category_2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "3e50dde6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3273</th>\n",
       "      <td>-0.018321</td>\n",
       "      <td>-0.052357</td>\n",
       "      <td>-0.018651</td>\n",
       "      <td>-0.047124</td>\n",
       "      <td>0.013237</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.077416</td>\n",
       "      <td>-0.041525</td>\n",
       "      <td>-0.002589</td>\n",
       "      <td>-0.015058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092750</td>\n",
       "      <td>0.060206</td>\n",
       "      <td>0.055140</td>\n",
       "      <td>-0.025249</td>\n",
       "      <td>-0.021813</td>\n",
       "      <td>0.081310</td>\n",
       "      <td>0.008720</td>\n",
       "      <td>-0.077827</td>\n",
       "      <td>-0.025864</td>\n",
       "      <td>0.024092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>-0.031328</td>\n",
       "      <td>0.035341</td>\n",
       "      <td>0.051224</td>\n",
       "      <td>0.009007</td>\n",
       "      <td>-0.126093</td>\n",
       "      <td>0.068837</td>\n",
       "      <td>0.077541</td>\n",
       "      <td>-0.005875</td>\n",
       "      <td>0.073230</td>\n",
       "      <td>-0.019220</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002163</td>\n",
       "      <td>0.050991</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.027465</td>\n",
       "      <td>-0.040250</td>\n",
       "      <td>0.036279</td>\n",
       "      <td>0.020731</td>\n",
       "      <td>-0.066904</td>\n",
       "      <td>-0.014063</td>\n",
       "      <td>-0.062386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.052998</td>\n",
       "      <td>0.003873</td>\n",
       "      <td>-0.033136</td>\n",
       "      <td>0.052382</td>\n",
       "      <td>-0.005269</td>\n",
       "      <td>0.013558</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>0.014771</td>\n",
       "      <td>-0.041350</td>\n",
       "      <td>0.032940</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076335</td>\n",
       "      <td>0.033985</td>\n",
       "      <td>-0.001367</td>\n",
       "      <td>-0.027902</td>\n",
       "      <td>0.031219</td>\n",
       "      <td>-0.040230</td>\n",
       "      <td>0.059878</td>\n",
       "      <td>-0.071884</td>\n",
       "      <td>-0.062483</td>\n",
       "      <td>0.053956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>-0.079986</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.075535</td>\n",
       "      <td>-0.063821</td>\n",
       "      <td>-0.042241</td>\n",
       "      <td>0.057083</td>\n",
       "      <td>-0.006113</td>\n",
       "      <td>-0.042671</td>\n",
       "      <td>0.141165</td>\n",
       "      <td>0.020186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100943</td>\n",
       "      <td>0.011537</td>\n",
       "      <td>0.012144</td>\n",
       "      <td>0.014254</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.066255</td>\n",
       "      <td>0.065277</td>\n",
       "      <td>-0.040160</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.053757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1825</th>\n",
       "      <td>0.008638</td>\n",
       "      <td>0.092242</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>0.011786</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>-0.051632</td>\n",
       "      <td>0.059525</td>\n",
       "      <td>-0.073021</td>\n",
       "      <td>0.010913</td>\n",
       "      <td>0.030849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021015</td>\n",
       "      <td>0.016952</td>\n",
       "      <td>-0.035221</td>\n",
       "      <td>-0.056255</td>\n",
       "      <td>0.055632</td>\n",
       "      <td>0.017367</td>\n",
       "      <td>-0.009059</td>\n",
       "      <td>-0.016018</td>\n",
       "      <td>-0.066937</td>\n",
       "      <td>0.002066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095</th>\n",
       "      <td>-0.009962</td>\n",
       "      <td>0.118509</td>\n",
       "      <td>-0.002937</td>\n",
       "      <td>0.027881</td>\n",
       "      <td>-0.021739</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>-0.011777</td>\n",
       "      <td>-0.042067</td>\n",
       "      <td>-0.016062</td>\n",
       "      <td>-0.038522</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031697</td>\n",
       "      <td>0.031880</td>\n",
       "      <td>-0.038804</td>\n",
       "      <td>0.011694</td>\n",
       "      <td>-0.034657</td>\n",
       "      <td>0.020018</td>\n",
       "      <td>0.096620</td>\n",
       "      <td>-0.047898</td>\n",
       "      <td>-0.080932</td>\n",
       "      <td>0.026744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1130</th>\n",
       "      <td>-0.000267</td>\n",
       "      <td>0.044392</td>\n",
       "      <td>-0.084331</td>\n",
       "      <td>-0.070166</td>\n",
       "      <td>-0.092035</td>\n",
       "      <td>0.110811</td>\n",
       "      <td>0.090509</td>\n",
       "      <td>-0.000427</td>\n",
       "      <td>-0.006379</td>\n",
       "      <td>-0.010363</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033917</td>\n",
       "      <td>-0.020676</td>\n",
       "      <td>-0.011809</td>\n",
       "      <td>0.066080</td>\n",
       "      <td>-0.003897</td>\n",
       "      <td>0.036833</td>\n",
       "      <td>-0.036978</td>\n",
       "      <td>0.024462</td>\n",
       "      <td>-0.027776</td>\n",
       "      <td>0.010644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1294</th>\n",
       "      <td>-0.065438</td>\n",
       "      <td>0.040576</td>\n",
       "      <td>-0.020869</td>\n",
       "      <td>-0.034447</td>\n",
       "      <td>-0.065259</td>\n",
       "      <td>0.017677</td>\n",
       "      <td>0.044606</td>\n",
       "      <td>0.039799</td>\n",
       "      <td>0.107024</td>\n",
       "      <td>0.111050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003349</td>\n",
       "      <td>0.049428</td>\n",
       "      <td>-0.014817</td>\n",
       "      <td>0.039712</td>\n",
       "      <td>0.041138</td>\n",
       "      <td>0.003920</td>\n",
       "      <td>0.066994</td>\n",
       "      <td>-0.092629</td>\n",
       "      <td>-0.032323</td>\n",
       "      <td>0.060776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.043927</td>\n",
       "      <td>-0.010038</td>\n",
       "      <td>-0.000662</td>\n",
       "      <td>-0.031824</td>\n",
       "      <td>0.013851</td>\n",
       "      <td>0.054518</td>\n",
       "      <td>-0.009458</td>\n",
       "      <td>-0.082221</td>\n",
       "      <td>-0.003457</td>\n",
       "      <td>0.039939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050679</td>\n",
       "      <td>0.041752</td>\n",
       "      <td>-0.055694</td>\n",
       "      <td>0.023295</td>\n",
       "      <td>-0.019691</td>\n",
       "      <td>0.073977</td>\n",
       "      <td>0.066427</td>\n",
       "      <td>-0.029612</td>\n",
       "      <td>-0.028127</td>\n",
       "      <td>0.024482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3174</th>\n",
       "      <td>-0.019756</td>\n",
       "      <td>0.066314</td>\n",
       "      <td>-0.053308</td>\n",
       "      <td>-0.078937</td>\n",
       "      <td>0.149074</td>\n",
       "      <td>0.046870</td>\n",
       "      <td>0.123806</td>\n",
       "      <td>-0.004389</td>\n",
       "      <td>0.116442</td>\n",
       "      <td>0.048242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.047039</td>\n",
       "      <td>-0.002282</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.078706</td>\n",
       "      <td>0.023735</td>\n",
       "      <td>-0.017208</td>\n",
       "      <td>-0.040195</td>\n",
       "      <td>0.038149</td>\n",
       "      <td>-0.024556</td>\n",
       "      <td>-0.000698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2366 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "3273 -0.018321 -0.052357 -0.018651 -0.047124  0.013237  0.006286  0.077416   \n",
       "900  -0.031328  0.035341  0.051224  0.009007 -0.126093  0.068837  0.077541   \n",
       "422   0.052998  0.003873 -0.033136  0.052382 -0.005269  0.013558  0.020203   \n",
       "887  -0.079986  0.008797 -0.075535 -0.063821 -0.042241  0.057083 -0.006113   \n",
       "1825  0.008638  0.092242  0.007948  0.011786  0.057034 -0.051632  0.059525   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1095 -0.009962  0.118509 -0.002937  0.027881 -0.021739  0.004040 -0.011777   \n",
       "1130 -0.000267  0.044392 -0.084331 -0.070166 -0.092035  0.110811  0.090509   \n",
       "1294 -0.065438  0.040576 -0.020869 -0.034447 -0.065259  0.017677  0.044606   \n",
       "860  -0.043927 -0.010038 -0.000662 -0.031824  0.013851  0.054518 -0.009458   \n",
       "3174 -0.019756  0.066314 -0.053308 -0.078937  0.149074  0.046870  0.123806   \n",
       "\n",
       "           7         8         9    ...       374       375       376  \\\n",
       "3273 -0.041525 -0.002589 -0.015058  ...  0.092750  0.060206  0.055140   \n",
       "900  -0.005875  0.073230 -0.019220  ... -0.002163  0.050991  0.000793   \n",
       "422   0.014771 -0.041350  0.032940  ... -0.076335  0.033985 -0.001367   \n",
       "887  -0.042671  0.141165  0.020186  ...  0.100943  0.011537  0.012144   \n",
       "1825 -0.073021  0.010913  0.030849  ...  0.021015  0.016952 -0.035221   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1095 -0.042067 -0.016062 -0.038522  ... -0.031697  0.031880 -0.038804   \n",
       "1130 -0.000427 -0.006379 -0.010363  ...  0.033917 -0.020676 -0.011809   \n",
       "1294  0.039799  0.107024  0.111050  ...  0.003349  0.049428 -0.014817   \n",
       "860  -0.082221 -0.003457  0.039939  ...  0.050679  0.041752 -0.055694   \n",
       "3174 -0.004389  0.116442  0.048242  ... -0.047039 -0.002282  0.000196   \n",
       "\n",
       "           377       378       379       380       381       382       383  \n",
       "3273 -0.025249 -0.021813  0.081310  0.008720 -0.077827 -0.025864  0.024092  \n",
       "900   0.027465 -0.040250  0.036279  0.020731 -0.066904 -0.014063 -0.062386  \n",
       "422  -0.027902  0.031219 -0.040230  0.059878 -0.071884 -0.062483  0.053956  \n",
       "887   0.014254  0.011945  0.066255  0.065277 -0.040160  0.002799  0.053757  \n",
       "1825 -0.056255  0.055632  0.017367 -0.009059 -0.016018 -0.066937  0.002066  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "1095  0.011694 -0.034657  0.020018  0.096620 -0.047898 -0.080932  0.026744  \n",
       "1130  0.066080 -0.003897  0.036833 -0.036978  0.024462 -0.027776  0.010644  \n",
       "1294  0.039712  0.041138  0.003920  0.066994 -0.092629 -0.032323  0.060776  \n",
       "860   0.023295 -0.019691  0.073977  0.066427 -0.029612 -0.028127  0.024482  \n",
       "3174  0.078706  0.023735 -0.017208 -0.040195  0.038149 -0.024556 -0.000698  \n",
       "\n",
       "[2366 rows x 384 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "97c0656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create an MLPClassifier model\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "# Fit the model on the training data\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Predict the target labels for the testing data\n",
    "mlp_prediction = mlp.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "77c0d486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9259338796080181"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the F1 score using weighted averaging\n",
    "f1 = f1_score(y_test, mlp_prediction, average='weighted')\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f96412f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9260355029585798"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, mlp_prediction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ae91eb",
   "metadata": {},
   "source": [
    "### Grid Search of MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ad194119",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'mlp__alpha': 0.001, 'mlp__hidden_layer_sizes': (100,), 'tfidf__max_df': 0.9, 'tfidf__min_df': 1}\n",
      "Best Score: 0.945054945054945\n",
      "F1 Score: 0.9507063442010122\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train, test = train_test_split(data_cleaned, random_state=42, test_size=0.3)\n",
    "\n",
    "# Define the feature and target columns\n",
    "X_train = train['Text_first200_translated_cleaned'] + ' ' + train['Title_Translated_cleaned']\n",
    "y_train = train['Category_2']\n",
    "X_test = test['Text_first200_translated_cleaned'] + ' ' + test['Title_Translated_cleaned']\n",
    "y_test = test['Category_2']\n",
    "\n",
    "# Create a pipeline with TfidfVectorizer and MLPClassifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('mlp', MLPClassifier())\n",
    "])\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'tfidf__max_df': [0.9, 0.95],\n",
    "    'tfidf__min_df': [1, 2],\n",
    "    'mlp__hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "#     'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "# Predict the target labels for the testing data using the best model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate the F1 score using weighted averaging\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Print the best parameters, best score, and F1 score\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"F1 Score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4fc62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Default parameters\n",
    "# MLPClassifier(\n",
    "#     hidden_layer_sizes=(100,),\n",
    "#     activation='relu',\n",
    "#     solver='adam',\n",
    "#     alpha=0.0001,\n",
    "#     batch_size='auto',\n",
    "#     learning_rate='constant',\n",
    "#     learning_rate_init=0.001,\n",
    "#     power_t=0.5,\n",
    "#     max_iter=200,\n",
    "#     shuffle=True,\n",
    "#     random_state=None,\n",
    "#     tol=1e-4,\n",
    "#     verbose=False,\n",
    "#     warm_start=False,\n",
    "#     momentum=0.9,\n",
    "#     nesterovs_momentum=True,\n",
    "#     early_stopping=False,\n",
    "#     validation_fraction=0.1,\n",
    "#     beta_1=0.9,\n",
    "#     beta_2=0.999,\n",
    "#     epsilon=1e-8,\n",
    "#     n_iter_no_change=10,\n",
    "#     max_fun=15000\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ddcda9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
